{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purchasing a home is one of the most  decisions people make. It is pivotal that a prospective home buyer makes this purchase at the correct price. However, when facing a decision of such financial magnitude, people may consider that they are paying more for the house than it’s worth. Previously the complexity of this task led people to simply take the price of the house and divide by the total square feet, obtaining a price per square foot which one could compare to other houses in the neighborhood.  Now with the data and increased computational power, we are able to increase the sophistication of our analysis based on a house’s features. However, because of different reasons, it is still necessary to extrapolate to the house actually being purchased. This project created models to find the best way of predicting housing prices in certain areas to make the process more transparent. The goal of this project is to enable house buyer having a better way of estimating house prices, and evaluating houses before purchasing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was compiled by Dean De Cock and published in https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview. The dataset under analysis was the House Prices: Advanced Regression Techniques set on Kaggle.com.  The location of the houses sold was in Ames, Iowa during the period 2006 to 2010.  There were 1460 houses and their sale prices for 79 features (e.g. the number of bathrooms, square footage, overall quality rating). The goal is to predict the prices of another set of houses for which the prices were not provided.\n",
    "Of the 79 explanatory variables, we found that 51 are categorical and 28 are continuous. We discovered that each predictor variable could be categorized into variables such as lot/land, location, age, appearance, external features, room/bathroom, kitchen, basement, roof, garage, and utilities, etc.\n",
    "Complete information about the variables can be found: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the total number of observations, I find there are 1460 in the train set, and 1459 in the test set. As Id is unnecessary for the prediction process so I remove it from both datasets. All the other columns except SalePrice will be fitted then in the models as features. Imputations to missing values will be talked about in the part of feature engeneering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing Id, I then take a look at outliers by plotting the relationship between GrLivArea and SalePrice. As it's shown below, the overall relationship turns to be positive, while there are two points with extremely large areas and very low prices. Based on the situation that larger area number usually leads to higher prices, it seems the two data points are for some reason recorded by mistake, so they are removed. <img src=\"pics/scatter1.png\" width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the relationship is like after removing the two dots: <img src=\"pics/scatter2.png\" width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to take a look at the dependent variable--SalePrice. As it is shown in the histogram, the distribution of prices looks a little right skewed, with the standard deviation huge. The probability plot confirms this observation (that the overall trend of quantiles doesn't follow a straight line):\n",
    "<img src=\"pics/saleprice1.png\" width='400'>\n",
    "<img src=\"pics/qq1.png\" width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As most models require normal distribution of the target variable, a log-transformation is added as a remedy to the situation. As it is shown below, the mean of all prices is 12.02 with .4 standard deviation, which indicating a normal distribution. The quantile plot also confirms this result, so in both datasets, log-transformation is applied to Saleprice.\n",
    "<img src=\"pics/saleprice2.png\" width='400'>\n",
    "<img src=\"pics/qq2.png\" width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do here is to investigate missing values. Missing values are detected combining the train and test datasets together. The target value Saleprice is ignored here. The type of imputation will largely depend on the amount of missing values for each feature, so the missing rates are shown below:\n",
    "<img src=\"pics/missing.png\" width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most categorical values in which large amount of values are missing such as PoolQC, MiscFeature, Alley and Fence, since there is not a specific reason for the amount of missing values, we just impute 'None' to be as a new level in the feature.\n",
    "\n",
    "For a numeric feature like LotFrontage, we use median to impute the column; For some ordinal features which have the value format of numbers but are categorical, we use the mode as well for the imputation. \n",
    "\n",
    "MSZoning and Electrical is categorical, but most of its values are 'RL'. so we impute 'RL' as well instead of 'none'.\n",
    "For the feature Utilities, we see all records shown in the column are 'AllPub', except for one 'NoSeWa' and two nas in the training dataset, which indicates this fearure won't help in predictive modelling, thus it's safely removed.\n",
    "\n",
    "According to the data description, the missing values in Functional mean 'typical', so just impute with the value.\n",
    "\n",
    "After all actions of imputation, we do need to transform some numerical variables to its own type, like changing year and month variable to categotical, though the values are numerical.\n",
    "\n",
    "To make sure the features will fit in each model, we apply label encoding for the categorical variables that may contain information in their ordering set.\n",
    "\n",
    "Also, a new feature called TotalSF is created representing the total square foot of a house, which is calculted basically by using the sum of all square feet for the 1st floor, the 2nd floor, and the basement.\n",
    "\n",
    "The last step for numeric features is to check the skewness. As we know, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. Using the skew function in python gives indices for each feature, the higher the value is, the skewer the data. Here is a preview of features that are highed skewed:\n",
    "<img src=\"skew.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
